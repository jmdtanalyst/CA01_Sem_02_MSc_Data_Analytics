{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install EMIST library, import datasets of letters, Matplotlib\n",
    "#!pip install emnist\n",
    "from emnist import list_datasets\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop previus context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "\n",
    "spark = pyspark.SparkContext(appName=\"Save EMNIST to HDFS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download dataset and it is 536 MB\n",
    "list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download images of letters from training samples or test samples\n",
    "#from emnist import extract_test_samples\n",
    "from emnist import extract_training_samples\n",
    "images, labels = extract_training_samples('letters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape, labels.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving on Hadoop via Pyspark\n",
    "    to save the dataset into Hadoop,  first it must be loaded into a RDD (Resilient Distributed Dataset) \n",
    "    It's a data structure in the Apache Spark framework designed for large-scale data processing on clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating rdd files\n",
    "# As spark works in partition, the number of partitions was increased to distribute the workload more evenly across tasks\n",
    "\n",
    "images_rdd = spark.parallelize(images)\n",
    "#images_rdd = images_rdd.repartition(12) \n",
    "\n",
    "labels_rdd = spark.parallelize(labels)\n",
    "#labels_rdd = images_rdd.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving on hadoop file system\n",
    "hdfs_path = \"hdfs://192.168.1.23:9000/emnist/\"\n",
    "# Save RDDs as parquet files in the specified HDFS path\n",
    "images_rdd.saveAsTextFile(hdfs_path+\"images\")\n",
    "labels_rdd.saveAsTextFile(hdfs_path+\"labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading RDD From hadoop\n",
    "    for handling large size data, Libraries like dask and modin should be used,  because they are designed to handle large datasets efficiently on a single machine or distributed cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = \"hdfs://localhost:9000/emnist/\"\n",
    "\n",
    "# Load data from HDFS using dask (replace 'path/to/data' with the actual location)\n",
    "images_rdd = spark.textFile(hdfs_path+\"images\")\n",
    "labels_rdd = spark.textFile(hdfs_path+\"labels\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot sample of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the RDD into 95% and 5% portions (adjust ratios as needed)\n",
    "training_data, sample_data = images_rdd.randomSplit([0.95, 0.05])\n",
    "\n",
    "# Collect the sample data\n",
    "sample_images = sample_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(3,5,figsize=(10,8))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    ax.imshow(sample_images[i].reshape([28,28]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocesssing\n",
    "## 1. Normalisation and reshaping of data\n",
    "    Before feeding in the data to the model we will normalise and reshape the data given to us. This will decrease the complexity of the models and make the model work efficiently as less complex numbers will be there to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encoding using keras' numpy-related utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Adding Callback API's to save best weights and change lrÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "# convolutional layer   - \n",
    "# ConvD(25 neurons)\n",
    "#kernel_size 3,3 = filter size =  3xx\n",
    "# strides =  how many square move by time  =   1 by on\n",
    "# padding = Valid\n",
    "# Activation function = relu  -  check ann to understand each activation function\n",
    "# input_shate =  size of 3d picture (28x28x1\n",
    "\n",
    "model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "\n",
    "# \n",
    "model.add(MaxPool2D(pool_size=(1,1)))\n",
    "\n",
    "# flatten output of conv\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "\n",
    "# hidden layer\n",
    "# Deense with 100 neurons   -activation relu -  check why not sigmoid\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# output layer\n",
    "#37 - the numbers from 0 to 9  and letters A-Z,  and actication softmax\n",
    "model.add(Dense(37, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compiling the sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Its a multiple categorical ,  so we user categorical_crossentropy as  less function.\n",
    "# optimizaer adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the model for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_size = get 128 data with and pass for the model 10 times (epochs)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('cnn_B.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from disk using later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('cnn_B.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on the first 5 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test [:10]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print our model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our predictions against the ground truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
