{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r /user/hduser/bike_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hduser/bike_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d003485",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_ingestion function: Loads all CSV files in a directory using PySpark and saves them to a specified output path on HDFS.\n",
    "# Args: \n",
    "    # path: Path to the directory containing CSV files  \n",
    "    # hd_path: Hadoop path on HDFS to save the processed files\n",
    "\n",
    "def data_ingestion( path, hd_path):\n",
    "    \n",
    "    try:\n",
    "        # Read all files in the directory matching the CSV extension (*.csv)\n",
    "        df = spark.read.csv(f\"{path}/*.csv\", header=True)\n",
    "\n",
    "        # Save the DataFrame to Hadoop server - HDFS\n",
    "        df.write.mode(\"overwrite\").csv(hd_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185609a8",
   "metadata": {},
   "source": [
    "# Running the data_ingestion fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSVProcessor\").getOrCreate()\n",
    "\n",
    "# path to directory where the files are\n",
    "path = \"file:///home/hduser/Documents/CA01_Sem_02_MSc_Data_Analytics\"\n",
    "\n",
    "# path where the files should be saved on hadoop\n",
    "hd_path = \"/user/hduser/bike_data\"\n",
    "\n",
    "# Call the processing function\n",
    "data_ingestion(path, hd_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e86915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3efcce0",
   "metadata": {},
   "source": [
    "# Load data from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3829c623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-------------------+-------------------+--------------------+-----+--------------------+-----+-----------------+------------------+-----------------+------------------+------+\n",
      "|             _c0|         _c1|                _c2|                _c3|                 _c4|  _c5|                 _c6|  _c7|              _c8|               _c9|             _c10|              _c11|  _c12|\n",
      "+----------------+------------+-------------------+-------------------+--------------------+-----+--------------------+-----+-----------------+------------------+-----------------+------------------+------+\n",
      "|7C7743DC7ACAABF0|classic_bike|2023-01-29 17:56:05|2023-01-29 18:08:43|Pentagon Row Plaz...|31006|Long Bridge Aquat...|31950|38.86331354301248|-77.06341624259947|38.87056104566178|-77.04409494996071|member|\n",
      "|5C93DC13029B47E4|classic_bike|2023-01-02 09:36:56|2023-01-02 09:54:56|Langston Blvd & N...|31080|      22nd & P ST NW|31285|        38.897612|        -77.080851|        38.909394|        -77.048728|member|\n",
      "|C41EFC8E89F5DA10|classic_bike|2023-01-28 17:12:08|2023-01-28 17:22:43|California St & F...|31116|      23rd & M St NW|31128|        38.917761|         -77.04062|        38.905303|        -77.050264|member|\n",
      "|31BEE2E626191816|classic_bike|2023-01-13 15:39:10|2023-01-13 15:50:07|      11th & S St NW|31280|      23rd & M St NW|31128|        38.913761|        -77.027025|        38.905303|        -77.050264|member|\n",
      "|E5B40CFA1C4CE993|classic_bike|2023-01-28 22:45:15|2023-01-28 22:55:53|      11th & S St NW|31280|      22nd & P ST NW|31285|        38.913761|        -77.027025|        38.909394|        -77.048728|casual|\n",
      "+----------------+------------+-------------------+-------------------+--------------------+-----+--------------------+-----+-----------------+------------------+-----------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hd_path = \"/user/hduser/bike_data\"\n",
    "\n",
    "try:\n",
    "   \n",
    "\n",
    "  # Read the CSV files using the specified path\n",
    "  df = spark.read.csv(hd_path)\n",
    "\n",
    "  # Print the first few rows of the DataFrame (optional)\n",
    "  df.show(5)\n",
    "\n",
    "   \n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
